{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from six import iteritems\n",
    "import xgboost as xgb\n",
    "from math import log\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "with open('train.txt', encoding='utf-8') as f:\n",
    "    train_data = f.readlines()\n",
    "    f.close\n",
    "with open('test.txt', encoding='utf-8') as f:\n",
    "    test_data = f.readlines()\n",
    "    f.close\n",
    "with open('Dream_of_the_Red_Chamber_seg.txt', encoding='utf-8') as f:\n",
    "    corpus_seg = f.readlines()\n",
    "    f.close\n",
    "with open('Dream_of_the_Red_Chamber.txt', encoding='utf-8') as f:\n",
    "    corpus = f.readlines()\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test & Train Array Converter\n",
    "TRAIN = []\n",
    "index = 0\n",
    "for row in train_data:\n",
    "    if index == 0:\n",
    "        index += 1\n",
    "        continue\n",
    "    index += 1\n",
    "    x = re.split('\\t|\\n', row)\n",
    "    TRAIN.append([x[1], x[2], x[3]])\n",
    "    \n",
    "TEST = []\n",
    "index = 0\n",
    "for row in test_data:\n",
    "    if index == 0:\n",
    "        index += 1\n",
    "        continue\n",
    "    index += 1\n",
    "    x = re.split('\\t|\\n', row)\n",
    "    TEST.append([x[1], x[2], x[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build The Word Vector\n",
    "TERM_LIST = []\n",
    "for paragraph in corpus_seg:\n",
    "    tokens = paragraph.split()\n",
    "    for token in tokens:\n",
    "        if '_P' in token:\n",
    "            continue\n",
    "        term = re.sub('_[A-Z | a-z | 0-9]*', '', token)\n",
    "        if term not in TERM_LIST:\n",
    "            TERM_LIST.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATION = {\n",
    "    '祖孫': 0,\n",
    "    '母子': 1,\n",
    "    '母女': 2,\n",
    "    '父子': 3,\n",
    "    '父女': 4,\n",
    "    '兄弟姊妹': 5,\n",
    "    '夫妻': 6,\n",
    "    '姑叔舅姨甥侄': 7,\n",
    "    '遠親': 8,\n",
    "    '主僕': 9,\n",
    "    '師徒': 10,\n",
    "    '居處': 11,\n",
    "}\n",
    "\n",
    "PERSON = {}\n",
    "index = 1\n",
    "for x in TRAIN:\n",
    "    per1 = x[0]\n",
    "    per2 = x[1]\n",
    "    if per1 not in PERSON:\n",
    "        PERSON[per1] = index\n",
    "        index += 1\n",
    "    if per2 not in PERSON:\n",
    "        PERSON[per2] = index\n",
    "        index += 1\n",
    "for x in TEST:\n",
    "    per1 = x[0]\n",
    "    per2 = x[1]\n",
    "    if per1 not in PERSON:\n",
    "        PERSON[per1] = index\n",
    "        index += 1\n",
    "    if per2 not in PERSON:\n",
    "        PERSON[per2] = index\n",
    "        index += 1\n",
    "        \n",
    "GENERALNAME = [\n",
    "    '婆子', '夫人', '大姐', '小姐', '嫂子', '姨娘',  '姨媽', '嬸娘', '嫂子', '老娘', '嬤嬤', '奶奶'\n",
    "]\n",
    "\n",
    "Rule1 = ['嫁','娶','婚','買','嫡夫','婦','嫡','妻','妾','連理','太太','夫妻']\n",
    "Rule2 = ['喚作','取名','生','有了','得了','養','懷','爹','娘','父','母','兒','女','女兒','子','孩','乳名','小名']\n",
    "Rule3 = ['請','給','來','請安','磕頭','問好','跪','稟明','奉','喚來','叫','祖','奶','孫','老太太','帶','領']\n",
    "Rule4 = ['長','次','大']\n",
    "Rule5 = ['兄','哥','弟','姊','姐','妹']\n",
    "Rule6 = ['姑','叔','舅','姨','甥','侄','親']\n",
    "Rule7 = ['帶','領','教','徒','門生','師父']\n",
    "Rule8 = ['主','僕','丫','丫頭','丫鬟','心腹','小的','下人','主僕']\n",
    "Rule9 = ['使喚','謝','領','接','扇','差','命','遣','迎','打發','吩咐','喚','罵']\n",
    "\n",
    "RULES = {\n",
    "    '婚配': Rule1, \n",
    "    '直系': Rule2, \n",
    "    '尊卑': Rule3, \n",
    "    '旁系': Rule4, \n",
    "    '手足': Rule5, \n",
    "    '遠親': Rule6, \n",
    "    '師徒': Rule7, \n",
    "    '主僕': Rule8,\n",
    "    '命令': Rule9\n",
    "}\n",
    "\n",
    "CORPUS = corpus\n",
    "\n",
    "PRIORITY = [8, 4, 2, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isGeneralName(name):\n",
    "    if len(name) == 2:\n",
    "        return False\n",
    "    else:\n",
    "        subname = name[1:]\n",
    "        if subname in GENERALNAME:\n",
    "            return True\n",
    "\n",
    "def isContainName(content, general_tag, name):\n",
    "    if name in content:\n",
    "        return True\n",
    "    elif(general_tag):\n",
    "        first_name = name[0]\n",
    "        last_name = name[1:]\n",
    "        if first_name in content and last_name in content:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, per1, per2, rel, file):\n",
    "        self.per1 = per1\n",
    "        self.per2 = per2\n",
    "        self.rel = rel\n",
    "        self.file = file\n",
    "        self.word_vector = {}\n",
    "        self.features = {}\n",
    "        self.initialize()\n",
    "    def initialize(self):\n",
    "        \n",
    "        # FEATURE0 - RELATION\n",
    "        self.features['關係'] = 12\n",
    "        \n",
    "        # Intialized the word vector\n",
    "        for term in TERM_LIST:\n",
    "            self.word_vector[term] = 0\n",
    "            \n",
    "        # Added Person Number(e.g. 曹雪芹: 1, 賈寶玉: 2...)\n",
    "        p1 =  PERSON[self.per2] if PERSON[self.per1] > PERSON[self.per2] else PERSON[self.per1]\n",
    "        p2 =  PERSON[self.per1] if PERSON[self.per1] > PERSON[self.per2] else PERSON[self.per2]\n",
    "        \n",
    "        # FEATURE1 - PER_1\n",
    "        # FEATURE2 - PER_2\n",
    "        self.features['角色一'] = p1\n",
    "        self.features['角色二'] = p2\n",
    "        \n",
    "        # FEATURE3 - LAST_NAME\n",
    "        # Determine the last name is same or not\n",
    "        if self.per1[0] == self.per2[0]:\n",
    "            self.features['姓'] = 100\n",
    "        else:\n",
    "            self.features['姓'] = 0\n",
    "            \n",
    "        #FEATURE4 ~ FEATURE13 - RULE\n",
    "        for feature, value in iteritems(RULES):\n",
    "            self.features[feature] = 0\n",
    "            \n",
    "    def extract(self, content, priority):\n",
    "        weight = priority\n",
    "        tokens = content.split()\n",
    "        # Relation in the content\n",
    "        if self.rel in tokens:\n",
    "            self.features['關係'] = RELATION[self.rel]\n",
    "        for token in tokens:\n",
    "            \n",
    "            if '_P' in token:\n",
    "                continue\n",
    "            elif '_DE' in token:\n",
    "                continue\n",
    "            elif '_T' in token:\n",
    "                continue\n",
    "            elif '_SHI' in token:\n",
    "                continue\n",
    "            \n",
    "            term = re.sub('_[A-Z | a-z | 0-9]*', '', token)\n",
    "            for feature, rule in iteritems(RULES):\n",
    "                weight = priority\n",
    "                if feature == '婚配':\n",
    "                    weight = priority\n",
    "                elif feature == '尊卑':\n",
    "                    weight = priority\n",
    "                if term in rule:\n",
    "                    self.features[feature] += 1 * weight                \n",
    "            self.word_vector[term] += 1 * priority\n",
    "    def save(self):\n",
    "        word_freq = ''\n",
    "        feature_str = ''\n",
    "        \n",
    "        for word, freq in iteritems(self.word_vector):\n",
    "            word_freq = word_freq + str(freq) + ','\n",
    "            \n",
    "        for feature, score in iteritems(self.features):\n",
    "            feature_str = feature_str + str(score) + ','\n",
    "            \n",
    "        #self.file.write(str(RELATION[self.REL]) + ',' + word_freq + \",\" + feature_str[:-1] + '\\n')\n",
    "        self.file.write(str(RELATION[self.rel]) + ',' + feature_str[:-1] + '\\n')\n",
    "        #self.file.write(str(RELATION[self.REL]) + ',' + word_freq[:-1] + '\\n')\n",
    "        \n",
    "        \n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, data, corpus, filename):\n",
    "        self.file = open(filename, 'w');\n",
    "        self.data = data\n",
    "        self.extracted = [None] * len(data)\n",
    "        self.corpus = corpus\n",
    "        self.vector = {}\n",
    "        \n",
    "    def close(self):\n",
    "        self.file.close();\n",
    "        \n",
    "    def transform(self):\n",
    "        index = 0\n",
    "        for row in self.data:\n",
    "            extracted = []\n",
    "            extractor = FeatureExtractor(row[0], row[1], row[2], self.file)\n",
    "            \n",
    "            per1 = row[0]\n",
    "            per2 = row[1]\n",
    "            per1_general = isGeneralName(per1)\n",
    "            per2_general = isGeneralName(per2)\n",
    "            \n",
    "            tag = False\n",
    "            \n",
    "            # Sentence\n",
    "            for paragraph in self.corpus:\n",
    "                sentences = re.split('，|。|？|！|；', paragraph)\n",
    "                for i in range(len(sentences)):\n",
    "                    if isContainName(sentences[i], per1_general, per1) and isContainName(sentences[i], per2_general, per2):\n",
    "                        extractor.extract(sentences[i], PRIORITY[0])\n",
    "                        extracted.append('S: ' + sentences[i])\n",
    "                        if (tag == False):\n",
    "                            tag = True\n",
    "            \n",
    "            # Context\n",
    "            if tag == False:\n",
    "                for paragraph in self.corpus:\n",
    "                    sentences = re.split('，|。|？|！|；', paragraph)\n",
    "                    for i in range(len(sentences)-2):\n",
    "                        context = sentences[i] + sentences[i+1] + sentences[i+2]\n",
    "                        if isContainName(context, per1_general, per1) and isContainName(context, per2_general, per2):\n",
    "                            extractor.extract(context, PRIORITY[1])\n",
    "                            extracted.append('C: ' + context)\n",
    "                            if (tag == False):\n",
    "                                tag = True\n",
    "            # Otherwise\n",
    "            if tag == False:\n",
    "                temp = ['', '']\n",
    "                for paragraph in self.corpus:\n",
    "                    sentences = re.split('，|。|？|！|；', paragraph)\n",
    "                    for sentence in sentences:\n",
    "                        if isContainName(sentence, per1_general, per1) and temp[0] == '':\n",
    "                            temp[0] = sentence\n",
    "                        if isContainName(sentence, per2_general, per2) and temp[1] == '':\n",
    "                            temp[1] = sentence\n",
    "                    if temp[0] != '' and temp[1] != '':\n",
    "                        extractor.extract(temp[0] + ' ' + temp[1], PRIORITY[3])\n",
    "                        extracted.append('O: ' + temp[0] + ' ' + temp[1])\n",
    "                        if tag == False:\n",
    "                            tag = True\n",
    "                        break;\n",
    "                        \n",
    "            # Otherwise\n",
    "            if tag == False:\n",
    "                temp = ['', '']\n",
    "                for paragraph in CORPUS:\n",
    "                    sentences = re.split('，|。|？|！|；', paragraph)\n",
    "                    for sentence in sentences:\n",
    "                        if per1 in sentence and temp[0] == '':\n",
    "                            temp[0] = sentence\n",
    "                        if per2 in sentence and temp[1] == '':\n",
    "                            temp[1] = sentence\n",
    "                    if temp[0] != '' and temp[1] != '':\n",
    "                        extracted_content = temp[0] + ' ' + temp[1]\n",
    "                        extracted.append('R: ' + extracted_content)\n",
    "                        break;\n",
    "                \n",
    "            self.extracted[index] = extracted\n",
    "            index +=1\n",
    "            extractor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rightRate=0.41071428571428603\n"
     ]
    }
   ],
   "source": [
    "generateFeatureFile(TRAIN, TEST, corpus_seg)\n",
    "xgboost_preds = xgboostTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_based_result = ruleBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.158679\n",
      "0.158679\n",
      "0.158679\n",
      "0.140799\n",
      "0.158679\n",
      "0.140799\n",
      "0.158679\n",
      "0.140799\n",
      "0.158679\n",
      "0.158679\n",
      "0.140799\n",
      "0.140799\n",
      "0.140799\n",
      "0.140799\n",
      "0.140799\n",
      "0.140799\n",
      "0.155635\n",
      "0.4910714285714286\n"
     ]
    }
   ],
   "source": [
    "result = mergeEvaluation(xgboost_preds, rule_based_result, 0.140624)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatureFile(train, test, corpus):\n",
    "    # Training Data & Testing Data Transformation\n",
    "    pre1 = Preprocessor(train, corpus, 'ftrain.txt')\n",
    "    pre1.transform()\n",
    "    pre1.close()\n",
    "\n",
    "    pre2 = Preprocessor(test, corpus, 'ftest.txt')\n",
    "    pre2.transform()\n",
    "    pre2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xgboostTraining():\n",
    "    dtrain = xgb.DMatrix('ftrain.txt')\n",
    "    dtest = xgb.DMatrix('ftest.txt')\n",
    "    # specify parameters via map\n",
    "    param = {}\n",
    "    # use softmax multi-class classification\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    # scale weight of positive examples\n",
    "    param['eta'] = 0.1126\n",
    "    param['max_depth'] = 2\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 12\n",
    "    num_round = 2\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    # make prediction\n",
    "    bst.save_model('temp.txt')\n",
    "    bst = xgb.Booster(param)\n",
    "    bst.load_model('temp.txt')\n",
    "    preds = bst.predict(dtest)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeEvaluation(xgboost_preds, rule_based_result, threshold):\n",
    "    error = 0\n",
    "    preds = xgboost_preds\n",
    "    result = rule_based_result\n",
    "    \n",
    "    for i in range(112):\n",
    "        prob = np.amax(preds[i])\n",
    "        label = preds[i].tolist().index(prob)\n",
    "        test_label = TEST[i][2]\n",
    "        if prob < threshold:\n",
    "            if result[i] != test_label:\n",
    "                error += 1;\n",
    "        else:\n",
    "            if (label != RELATION[test_label]):\n",
    "                error += 1;\n",
    "                print(prob)\n",
    "    return(1 - error/112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus,term_dic): #Create function (arg1,arg2....)\n",
    "    # treat every paragraph as a document : N=num_of_para\n",
    "    num_of_para=len(corpus)\n",
    "    temp_dic={}\n",
    "    for term in term_dic:#有的兩個最終都=0，為什麼？？！？！\n",
    "        doc_freq=0\n",
    "        term_freq=0\n",
    "        for paragraph in corpus:\n",
    "            if (paragraph.find(term)>=0):\n",
    "                doc_freq+=1\n",
    "                term_freq+=paragraph.count(term)     \n",
    "        #weight=(1+math.log10(term_freq))*(math.log10(num_of_para/doc_freq))\n",
    "        if doc_freq==0:\n",
    "            weight=0\n",
    "        else:\n",
    "            weight=(1+math.log10(term_freq))*math.log10(num_of_para/doc_freq)\n",
    "        temp_dic[term] = weight\n",
    "    return temp_dic\n",
    "\n",
    "def ruleBase():\n",
    "    # Build The Term Dictinary\n",
    "    term_dic = {} # {term:代號,...}\n",
    "    term_weight_dic = {}\n",
    "    \n",
    "    for paragraph in corpus_seg:\n",
    "        tokens = paragraph.split()\n",
    "        for token in tokens:\n",
    "            # Normal Norm\n",
    "            if '_Na' in token:\n",
    "                pair = token.split('_')\n",
    "                if pair[0] not in term_dic:\n",
    "                    term_dic[pair[0]]=pair[1]\n",
    "            # 沒有加Nb: 專有名詞！\n",
    "            # Location\n",
    "            elif '_Nc' in token:\n",
    "                pair = token.split('_')\n",
    "                if pair[0] not in term_dic:\n",
    "                    term_dic[pair[0]]=pair[1]\n",
    "            # Time\n",
    "            elif '_Nd' in token:\n",
    "                pair = token.split('_')\n",
    "                if pair[0] not in term_dic:\n",
    "                    term_dic[pair[0]]=pair[1]\n",
    "            elif '_V' in token:\n",
    "                pair = token.split('_')\n",
    "                if pair[0] not in term_dic:\n",
    "                    term_dic[pair[0]]=pair[1]\n",
    "                    \n",
    "    term_weight_dic = tf_idf(corpus,term_dic)\n",
    "\n",
    "    featureDic={}\n",
    "    featureDic['婚配']=['嫁','娶','婚','買','嫡夫','婦','嫡','妻','妾','連理','太太','夫妻']\n",
    "    featureDic['直系']=['喚作','取名','生','有了','得了','養','懷','爹','娘','父','母','兒','女','女兒','子','孩','乳名','小名']\n",
    "    featureDic['尊卑']=['請','給','來','請安','磕頭','問好','跪','稟明','奉','喚來','叫','祖','奶','孫','老太太','帶','領']\n",
    "    featureDic['旁系']=['長','次','大']\n",
    "    featureDic['手足']=['兄','哥','弟','姊','姐','妹']\n",
    "    featureDic['遠親']=['姑','叔','舅','姨','甥','侄','親']\n",
    "    featureDic['師徒']=['帶','領','教','徒','門生','師父']\n",
    "    featureDic['主僕']=['主','僕','丫','丫頭','丫鬟','心腹','小的','下人','主僕']\n",
    "    featureDic['命令']=['使喚','謝','領','接','扇','差','命','遣','迎','打發','吩咐','喚','罵']\n",
    "    featureDic['地點']=[]\n",
    "    for key in term_dic:\n",
    "        if 'Nc' in term_dic[key]:\n",
    "            featureDic['地點'].append(key)\n",
    "    relationDic={'祖孫':0, '母子':1, '母女':2, '父子':3, '父女':4, '兄弟姊妹':5,'夫妻':6,\n",
    "                 '姑叔舅姨甥侄':7,'遠親':8,'主僕':9, '師徒':10,'居處':11 }\n",
    "    featureDic['女性']=['嬤','母','姐','姊','妹','太','夫人','氏','娘','女','姑','姨']\n",
    "    # 若冠夫姓或父姓，可能會出現的稱呼\n",
    "    featureDic['父姓']=['姐','母','娘','媽','奶','嬤']\n",
    "\n",
    "\n",
    "    # store data to judge\n",
    "    train = [] # form:[[E1 E2 R],[E1,E2,R]......]\n",
    "    index = 0\n",
    "    for row in test_data:\n",
    "        if index == 0:\n",
    "            index += 1\n",
    "            continue\n",
    "        index += 1\n",
    "        x = re.split('\\t|\\n', row)\n",
    "        # E1 E2 R\n",
    "        train.append([x[1], x[2], x[3]])\n",
    "\n",
    "    preFile = open('Segpreprocess.txt','w') \n",
    "\n",
    "    rightRelation=[]\n",
    "    judgeRelation=[]\n",
    "    # search corpus by two entities and relationship\n",
    "    for row in TEST:\n",
    "        oneLine=[]\n",
    "        oneSentence=[]\n",
    "        threeSentences=[]\n",
    "        oneParagraph=[]\n",
    "        # 待補：上一段末句和下一段首句\n",
    "\n",
    "        rightRelation.append(row[2])\n",
    "\n",
    "        # 其中一個是地方就不用做feature list判斷了，一定是居處\n",
    "        if (row[0] in featureDic['地點']) or (row[1] in featureDic['地點']):\n",
    "            judgeRelation.append('居處')        \n",
    "            continue\n",
    "        # 三個字人名如果有姓，去掉比較好找。若最後一個字是「娘」代表是姨娘，姓不可以省略\n",
    "        else:\n",
    "            entity1=row[0]\n",
    "            entity2=row[1]\n",
    "            if (len(row[0])==3 and ('娘' not in row[0]) and ('嬤' not in row[0])):\n",
    "                entity1=row[0][1:]\n",
    "            if (len(row[1])==3 and ('娘' not in row[0]) and ('嬤' not in row[0])):\n",
    "                entity2=row[1][1:]\n",
    "\n",
    "\n",
    "        preFile.write(row[0]+' '+row[1]+' '+row[2]+'\\n')\n",
    "\n",
    "        for paragraph in corpus:\n",
    "            #paragraph.replace(\" \",\"\")\n",
    "            if ((entity1 in paragraph) and (entity2 in paragraph)):\n",
    "                inLine = False\n",
    "                inSentence = False\n",
    "                inThreeSentences = False\n",
    "                lines = re.split('[，；。？！]', paragraph)\n",
    "                for line in lines:\n",
    "                    if ((entity1 in line) and (entity2 in line)):\n",
    "                        inLine = True\n",
    "                        oneLine.append(line)\n",
    "                        preFile.write(\"LINE:\"+line+'\\n')\n",
    "\n",
    "                sentences = re.split('[。？！]', paragraph)\n",
    "                thrSentences = []\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    idx=sentences.index(sentence)\n",
    "                    # create 3-sentences group\n",
    "                    if idx>1:\n",
    "                        thrSentences.append(sentences[idx-2]+\"。\"+sentences[idx-1]+\"。\"+sentences[idx])#中間標點統一以。代替\n",
    "                    # judge if in the list\n",
    "                    if ((entity1 in sentence) and (entity2 in sentence)):\n",
    "                        inSentence = True\n",
    "                        # 不能跟前面重複，中間沒逗點再加\n",
    "                        commaLoc=[m.start() for m in re.finditer('[，；]', sentence)] # get all locations of '，；' \n",
    "                        hasCommaBetween = False\n",
    "                        for cLoc in commaLoc:#暫不考慮同一人名一句話出現兩次的特例\n",
    "                            a = sentence.find(row[0])\n",
    "                            b = sentence.find(row[1])\n",
    "                            if ((a<cLoc and cLoc<b) or (b<cLoc and cLoc<a)):\n",
    "                                hasCommaBetween = True\n",
    "                                break\n",
    "                        if (hasCommaBetween == True):\n",
    "                            oneSentence.append(sentence)\n",
    "                            preFile.write(\"SENTENCE:\"+sentence+'\\n')\n",
    "\n",
    "                for context in thrSentences:\n",
    "                    if ((entity1 in context) and (entity2 in context)):\n",
    "                        inThreeSentences = True\n",
    "                        # 不能跟前面重複，中間沒。再加\n",
    "                        periodLoc=[m.start() for m in re.finditer('[。]', context)] # get all locations of '。' \n",
    "\n",
    "                        hasPeriodBetween = False\n",
    "                        for pLoc in periodLoc:#暫不考慮同一人名一句話出現兩次的特例\n",
    "                            a = context.find(entity1)\n",
    "                            b = context.find(entity2)\n",
    "                            if ((a<pLoc and pLoc<b) or (b<pLoc and pLoc<a)):\n",
    "                                hasPeriodBetween = True\n",
    "                                break\n",
    "                        if (hasPeriodBetween == True):\n",
    "                            threeSentences.append(context)\n",
    "                            preFile.write(\"CONTEXT:\"+context+'\\n')\n",
    "\n",
    "\n",
    "                if not (inLine or inSentence or inThreeSentences):\n",
    "                    oneParagraph.append(paragraph)\n",
    "                    preFile.write(\"PARAGRAPH:\"+paragraph+'\\n')\n",
    "\n",
    "        # create a dictionary to store appear phrase weight            \n",
    "        term_weight_vector={}\n",
    "\n",
    "        for line in oneLine:\n",
    "            tempLine = line\n",
    "            for term in term_dic:\n",
    "                if term in tempLine:            \n",
    "                    if term not in term_weight_vector:\n",
    "                        term_weight_vector[term]=0\n",
    "                    term_weight_vector[term]+=tempLine.count(term)*16\n",
    "                    #s = '_'+term_dic[term]\n",
    "                    #tempLine = tempLine.replace(term,s)\n",
    "        for context in threeSentences:\n",
    "            tempContext = context\n",
    "            for term in term_dic:\n",
    "                if term in tempContext:            \n",
    "                    if term not in term_weight_vector:\n",
    "                        term_weight_vector[term]=0\n",
    "                    term_weight_vector[term]+=tempContext.count(term)*4\n",
    "        for sentence in oneSentence:\n",
    "            tempSentence = sentence\n",
    "            for term in term_dic:\n",
    "                if term in tempSentence:            \n",
    "                    if term not in term_weight_vector:\n",
    "                        term_weight_vector[term]=0\n",
    "                    term_weight_vector[term]+=tempSentence.count(term)*2\n",
    "        for paragraph in oneParagraph:\n",
    "            tempParagraph = paragraph\n",
    "            for term in term_dic:\n",
    "                if term in tempParagraph:            \n",
    "                    if term not in term_weight_vector:\n",
    "                        term_weight_vector[term]=0\n",
    "                    term_weight_vector[term]+=tempParagraph.count(term)*1\n",
    "        #print (term_weight_vector)\n",
    "\n",
    "        # create feature list that symbolize different relationship\n",
    "        featureList=[0]*12\n",
    "        for term in term_weight_vector:\n",
    "            if term in featureDic['婚配']:\n",
    "                featureList[relationDic['夫妻']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['直系']:\n",
    "                featureList[relationDic['父子']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['父女']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['母子']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['母女']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['尊卑']:\n",
    "                featureList[relationDic['祖孫']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['主僕']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['夫妻']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['旁系']:\n",
    "                featureList[relationDic['兄弟姊妹']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['手足']:\n",
    "                featureList[relationDic['兄弟姊妹']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['主僕']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['主僕']:\n",
    "                featureList[relationDic['主僕']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['命令']:\n",
    "                featureList[relationDic['夫妻']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['主僕']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['父子']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['父女']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['母子']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "                featureList[relationDic['母女']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['遠親']:\n",
    "                featureList[relationDic['遠親']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "            elif term in featureDic['師徒']:\n",
    "                featureList[relationDic['師徒']]+=term_weight_vector[term]*term_weight_dic[term]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 偵測是否冠夫姓或父姓\n",
    "        hasMaleHead=False\n",
    "        for f in featureDic['父姓']:\n",
    "            if (f in row[0]) or (f in row[1]):\n",
    "                hasMaleHead=True\n",
    "                break\n",
    "\n",
    "        # 同姓：不會是主僕（僕人通常是暱稱），且更有可能是父子、父女、祖孫、兄弟姊妹、姑叔舅姨甥侄、遠親\n",
    "        if (row[0][0]==row[1][0]):\n",
    "            featureList[relationDic['主僕']]=0\n",
    "\n",
    "            featureList[relationDic['遠親']]+=100\n",
    "            featureList[relationDic['父子']]+=100 # 數字我亂設的\n",
    "            featureList[relationDic['父女']]+=100\n",
    "            featureList[relationDic['祖孫']]+=100\n",
    "            featureList[relationDic['兄弟姊妹']]+=100\n",
    "            featureList[relationDic['姑叔舅姨甥侄']]+=100\n",
    "\n",
    "            # 若冠夫姓或父姓，同姓仍有可能是母子或母女\n",
    "            if not hasMaleHead:\n",
    "                featureList[relationDic['母子']]=0\n",
    "                featureList[relationDic['母女']]=0\n",
    "            else:\n",
    "                featureList[relationDic['母子']]+=100\n",
    "                featureList[relationDic['母女']]+=100\n",
    "\n",
    "        # 偵測兩人中是否有女性\n",
    "        hasFemale=False\n",
    "        for f in featureDic['女性']:\n",
    "            if (f in row[0]) or (f in row[1]):\n",
    "                hasFemale=True\n",
    "                break\n",
    "\n",
    "        # 有女性就不會是父子、師徒\n",
    "        if hasFemale:\n",
    "            featureList[relationDic['父子']]=0\n",
    "            featureList[relationDic['師徒']]=0\n",
    "\n",
    "\n",
    "        # 還沒設想值相同的狀況\n",
    "        max_value=featureList.index(max(featureList))\n",
    "\n",
    "\n",
    "        #judgeRelation.append(relationDic.keys()[relationDic.values().index(max_value)])\n",
    "        judgeRelation.append(list(relationDic.keys())[list(relationDic.values()).index(max_value)])\n",
    "    preFile.close()            \n",
    "\n",
    "    rightRate=0\n",
    "    for i in range(len(judgeRelation)):\n",
    "        if judgeRelation[i]==rightRelation[i]:\n",
    "            #print(judgeRelation[i]+\"=\"+rightRelation[i])\n",
    "            rightRate+=1/len(judgeRelation)\n",
    "    print (\"rightRate=\"+str(rightRate))\n",
    "    return judgeRelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
